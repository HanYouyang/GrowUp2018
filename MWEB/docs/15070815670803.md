# 深度学习概念

**深度学习**：“层次化的概念让计算机构建较简单的概念来学习复杂概念。如果绘制出表示这些概念如何建立在彼此之上的一幅图，我们将得到一张‘深’（层次很多）的图。由此，我们称这种方法为AI深度学习（deep learning）”。

一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码（hard-code）。计算机可以使用逻辑推理规则来自动地理解这些形式化语言中的声明。这就是众所周知的人工智能的**知识库**（knowledge base）方法。

依靠硬编码的知识体系面临的困难表明，AI系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。这种能力称为**机器学习**（ machine learning）。

解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为**表示学习** （representation learning）。

表示学习算法的典型例子是**自编码器** （autoencoder），由一个编码器 （encoder）函数和一个解码器 （decoder）函数组合而成。编码器函数将输入数据转换为一种不同的表示，而解码器函数则将这个新的表示转换回原来的形式。

当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的**变差因素** （factors of variation）。在此背景下，“因素”这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。

深度学习模型的典型例子是前馈深度网络或或**多层感知机** （multilayer perceptron，MLP）。多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。

入展示在 **可见层** （visible layer），这样命名的原因是因为它包含我们能观察到的变量。然后是一系列从图像中提取越来越多抽象特征的 **隐藏层** （hidden layer）。因为它们的值不在数据中给出，所以将这些层称为“隐藏层”；模型必须确定哪些概念有利于解释观察数据中的关系。

## 第2章　线性代数

标量 （**scalar**）：一个标量就是一个单独的数。

向量 （**vector**）：一个向量是一列数。

矩阵 （**matrix**）：矩阵是一个二维数组，其中的每一个元素由两个索引（而非一个）所确定。

张量 （**tensor**）：在某些情况下，我们会讨论坐标超过两维的数组，一般的，一个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。

转置 （**transpose**）是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为主对角线 （**main diagonal**）。


向量 b 和矩阵 A 的每一行相加。这个简写方法使我们无须在加法操作前定义一个将向量 b 复制到每一行而生成的矩阵。这种隐式地复制向量 b 到很多位置的方式，称为广播 （**broadcasting**）。

两个矩阵 A 和 B 的矩阵乘积 （**matrix product**）是第三个矩阵 C 。为了使乘法可被定义，矩阵 A 的列数必须和矩阵 B 的行数相等。

两个矩阵的标准乘积不是指两个矩阵中对应元素的乘积。不过，那样的矩阵操作确实是存在的，称为元素对应乘积 （element-wise product）或者Hadamard乘积 （Hadamard product），记为 A.B 。两个相同维数的向量 x 和 y 的点积 （**dot product**）可看作矩阵乘积。
![屏幕快照 2017-10-04 10.47.25](media/15070815670803/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-10-04%2010.47.25.png)
![屏幕快照 2017-10-04 10.47.31](media/15070815670803/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-10-04%2010.47.31.png)
![屏幕快照 2017-10-04 10.47.36](media/15070815670803/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-10-04%2010.47.36.png)

单位矩阵 （**identity matrix**）：任意向量和单位矩阵相乘，都不会改变，结构为所有沿主对角线的元素都是1，而其他位置的所有元素都是0。

![屏幕快照 2017-10-04 10.51.10](media/15070815670803/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-10-04%2010.51.10.png)


当逆矩阵 A −1 存在时，有几种不同的算法都能找到它的闭解形式。理论上，相同的逆矩阵可用于多次求解不同向量b的方程。然而，逆矩阵 A −1 主要是作为理论工具使用的，并不会在大多数软件应用程序中实际使用。这是因为逆矩阵 A −1 在数字计算机上只能表现出有限的精度，有效使用向量 b 的算法通常可以得到更精确的 x 。


**线性组合** （linear combination）。形式上，一组向量的线性组合，是指每个向量乘以对应标量系数之后的和，即一组向量的**生成子空间** （span）是原始向量线性组合后所能抵达的点的集合。确定 Ax=b 是否有解，相当于确定向量 b 是否在 A 列向量的生成子空间中。这个特殊的生成子空间被称为 A 的**列空间** （column space）或者 A 的**值域** （range）。

**线性相关** （linear dependence）。如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为**线性无关** （linearly independent）。

要想使矩阵可逆，我们还需要保证式（2.11）对于每一个 b 值至多有一个解。为此，我们需要确保该矩阵至多有m个列向量。否则，该方程会有不止一个解。综上所述，这意味着该矩阵必须是一个**方阵** （square），即m=n，并且所有列向量都是线性无关的。一个列向量线性相关的方阵被称为**奇异**的 （singular）

如果矩阵 A 不是一个方阵或者是一个奇异的方阵，该方程仍然可能有解。但是我们不能使用矩阵逆去求解。


**范数**（包括L p 范数）是将向量映射到非负值的函数。直观上来说，向量 x 的范数衡量从原点到点 x 的距离。更严格地说，范数是满足下列性质的任意函数：




















































































































